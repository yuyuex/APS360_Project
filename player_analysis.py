# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EF1N2c1QC6AfYaF4Vlw9VsnfxAzrFYJI

**TO DO**

Separate the data to smaller chunks. Make sure that the slicing is done based on the match_id number. 

Just separate by season
"""

import os
import pandas as pd
import numpy as np
from google.colab import drive
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.utils.data as data_utils
import matplotlib.pyplot as plt

drive.mount('/content/gdrive')
data_set = pd.read_csv("../content/gdrive/My Drive/Colab Notebooks/Torneo/nba_summary_17_19.csv")
data_box_score = pd.read_csv("../content/gdrive/My Drive/Colab Notebooks/Torneo/nba_team_boxscores_2017-2019.csv")
from sklearn import preprocessing
import warnings
warnings.filterwarnings('ignore')

print(data_set.columns)
print(data_set.ft_pct.mean())

analyze()

def usg_deviation(df):
  df["usg_mean"]=df.normal_usg.mean()
  df["usg_std"]=df.normal_usg.std()
  df["usg_deviation"]=(df["normal_usg"]-df["usg_mean"])/df["usg_std"]
  return df

#print(data_set.columns)
print(list(set(data_set.season)))
data_set=data_set[data_set.season=='2018-19']

team_attr=["match_id","team_id","player_id","fg_pct","ft_pct","oreb","fg3_pct","ast_pct","ast_ratio","e_def_rating","e_off_rating","final_hscore","final_vscore","min"]
const_attr=["match_id","team_id","final_hscore","final_vscore","min"]
vars_attr=variable_attrs()
player1=data_set[data_set.player_id==201935].loc[:,vars_attr]
#print(player1.head())
player_avg=inference_df()
team_avg(player_avg)
#data_set.head()

def variable_attrs():
  vars_attr=[attr for attr in team_attr if attr not in const_attr]
  return vars_attr

def inference_df():
  global data_set
  data_set2=data_set[data_set.season=="2017-18"]
  data_trim=data_set2.loc[:,team_attr]
  data_trim=data_trim.set_index(const_attr)
  player_running_avg=data_trim.groupby("player_id").apply(lambda x:x.rolling(window=5).mean())

  print(len(player_running_avg))
  
  # Remove nan
  player_running_avg.dropna(inplace=True)
  #print(player_running_avg.head())
  return player_running_avg

def team_avg(player_running_avg):
  vars_attr.remove("player_id")
  player_running_avg=player_running_avg.reset_index()
  # Remove the records where the player didn't play
  player_running_avg["played"]=player_running_avg["min"]>0
  player_running_avg=player_running_avg[player_running_avg.played==True]
  const_attr.append("played")
  # Normalize so that the sum of minutes played will be 1
  # This method takes care of missing players
  # If the
  player_running_avg["norm_min"]=player_running_avg["min"]
  print(player_running_avg["norm_min"])

  #apply returns series
  #Convert to dataframe
  # The indexes  change since in the previous case there were records 
  # That we removed. normalized_min has indices that just run linearly
  # We have to convert the column to a list so the indices are ignored
  # Otherwise it won't match
  normalized_min=player_running_avg.groupby(["match_id","team_id"]).apply(lambda x:x.norm_min/x.norm_min.sum())
  normalized_min=pd.DataFrame(normalized_min)  
  normalized_min=normalized_min.reset_index()
  this_column=list(pd.Series(normalized_min.norm_min))
  const_attr.append("norm_min")
  player_running_avg["norm_min"]=this_column
  
  #.mul multiplies everything! including the indices
  # So those columns that are not meant to change should be saved
  # And re-assigned
  saved_df=player_running_avg.loc[:,const_attr]
  result = player_running_avg.mul(player_running_avg["norm_min"], axis=0)
  result[const_attr]=saved_df
  
  
  c_const_attr=const_attr.copy()
  c_const_attr.append("player_id")
  c_const_attr.remove("team_id")
  c_const_attr.remove("match_id")
  player_running_avg=player_running_avg[player_running_avg["min"]!=0]
  player_running_avg=player_running_avg.set_index(c_const_attr)

  team_avg=player_running_avg.groupby(["match_id","team_id"]).agg("mean")
  team_avg.dropna(inplace=True)
  
  print(team_avg.head())

running_avg(2)

def running_avg(window_size):
  df = pd.DataFrame({'A':list(np.arange(1,11)),'B': list(np.arange(10)),'C':list(np.arange(10))})
  df=df.set_index("A")
  res=df.rolling(window=window_size).mean()
  res=res.reset_index()
  print(res)

def analyze():
  global data_set
  #print(data_set.columns)

  #usage_percent=data_set[(data_set.match_id==data_set.loc[0,"match_id"]) & (data_set.team_abbr=="HOU")].usg_pct
  #usage_percent=usage_percent/usage_percent.sum()
  #usage_percent.sum()


  #data_set=data_set.groupby(["team_id","match_id"]).apply(normalize_usg_pct)
  data_set=data_set.groupby(["player_id"]).apply(usg_deviation)

  graph(data_set.usg_deviation,data_set.e_off_rating)

def graph(x,y):
  usg=data_set.normal_usg
  e_off=data_set.e_def_rating
  plt.scatter(x, y)
  plt.title('usage deviation vs offensive rating')
  plt.xlabel('usage deviation')
  plt.ylabel('offensive rating')
  plt.show()

def normalize_usg_pct(df):
  df["normal_usg"]=df["usg_pct"]/df["usg_pct"].sum()
  return df

def remove_cols():
  redun_cols=["created_on","updated_on","source","team_city"]